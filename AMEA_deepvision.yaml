net_name: 'AMEA_deepvision'
train_dir: '/home/data/LiTS/fixed_data/'
train_list: '/home/xzy/projects/BrainSeg/datatxt/LitsSeg_train.txt'
test_dir: '/home/data/LiTS/fixed_data/'
test_list: '/home/xzy/projects/BrainSeg/datatxt/LitsSeg_test.txt'
depth: 24

pretrain_model: '/home/xzy/projects/BrainSeg/checkpoints/BrainSeg/AMEA_deepvision/epoch_70_model_final.pth'
resume_model: '/home/xzy/projects/BrainSeg/checkpoints/BrainSeg/AMEA_deepvision/epoch_70_model_final.pth'
input_height: 256
input_width: 256
train_epoch : 100
save_every_epoch : 5
# change the batchsize
batch_size: 1 # 32
print_freq : 100
last_iter : -1 
augment: True
sync: True
train_output_directory: 'checkpoints/LitsSeg/AMEA_deepvision'
log_directory: 'logs/AMEA_deepvision'
savedir: 'checkpoints/save'
caffe_dir: checkpoints/caffe_model/LitsSeg
onnx_dir: checkpoints/onnx_model/LitsSeg

bn: 'sync'

model:
  in_channels: 1
  out_channels: 2


optimizer:
  type: 'adam'
  
lr_scheduler:
  base_lr: 0.001
  type: STEP                  #or COSINE
  gamma: 0.1                  #default for STEP
  verbose: False               #default for STEP
  step_size: 60              #default for STEP
  last_epoch: -1       


loss:
  type: 'bcel'
  # kwargs:
  #   class_num: 2

